# Systems-Analisis-Desing-Team-22

---

# American Express – Default Prediction (Systems Analysis)

---

# WorkShop 1
For the Overview section, we examined the description of the competition on Kaggle, with the aim of predicting customer default risk. Emphasis was placed on the use of a dataset of anonymized transactions, the binary default variable, and a special evaluation metric. The information was then organized into clear text that connects the technical aspects with their importance in financial risk management.

---

# Systemic Analysis

This repository contains a systemic analysis of the American Express Default Prediction competition on Kaggle.  
The analysis was carried out following a systemic thinking approach, where the key elements of the system, their relationships and flows were identified, and subsequently, complexity, sensitivity, as well as the presence of chaos and randomness were evaluated.

---

# 1. Identification of Key Elements

The main nodes that make up the system were defined: clients and their time series, account records, storage in the Raw Data Lake and Data Warehouse, preprocessing processes and data pipelines, predictive models based on ensembling and stacking, evaluation metrics, production implementation, and monitoring.

---

## 2. Mapping of Relationships and Flows

The flow of information between the different nodes was analyzed:
- Data flows from the Raw Data Lake into the Data Warehouse.
- In Preprocessing, data is cleaned and later consolidated through Batch Processing.
- New features are generated during Feature Engineering.
- Models are trained using ensembling and stacking techniques.
- Evaluation is conducted via stratified validation, Gini metrics, and overfitting control.
- Once approved, the model is deployed into production.
- The Monitoring Dashboard supervises real-time performance.
- A feedback loop exists: findings from the evaluation phase (e.g., seed control or overfitting) return to Feature Engineering, enabling continuous model improvement.

---

## 3. Complexity and Sensitivity

The factors that increase the system’s complexity were determined: temporality and sequential dependencies in clients, high dimensionality of variables, loss of interpretability due to anonymization, and scalability challenges from the large volume of data. Likewise, critical parameters that directly impact performance were identified: the time aggregation window, client distribution, data type compression and conversion, environment and library versions, and the definition of monitoring thresholds.

---

## 4. Chaos and Randomness

Potential sources of nonlinear and unpredictable behavior were explored: human variations and external events, feedback loops generated by model decisions in production, and complex interactions among variables that may trigger abrupt shifts in default risk.

---

# Conlusion

The system shows strengths in its modular architecture, advanced feature engineering, and robust ensemble modeling, reinforced by strong validation and monitoring practices. However, it is also highly sensitive to parameter tuning, data distribution shifts, and external randomness. Its long-term success relies on continuous monitoring, feedback loops, and rigorous control of variability across the pipeline.

---

# PDF Link
https://udistritaleduco-my.sharepoint.com/:b:/g/personal/jpgalindof_udistrital_edu_co/EYCwIrWb1nFApX6X48ybRGIBcf-NH6_x6yq9oye_RDNAug?e=FYWxPS








